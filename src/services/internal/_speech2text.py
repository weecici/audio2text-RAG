import torch
import os
from pathlib import Path
from functools import lru_cache
from typing import Union
from src.core import config
from src.utils import logger
from faster_whisper import WhisperModel, BatchedInferencePipeline


@lru_cache(maxsize=1)
def _get_s2t_batched_model() -> BatchedInferencePipeline:
    logger.info(
        f"Loading speech to text model: faster-whisper-{config.SPEECH2TEXT_MODEL}"
    )
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = WhisperModel(
        config.SPEECH2TEXT_MODEL, device=device, compute_type="float16"
    )
    batched_model = BatchedInferencePipeline(model=model)
    return batched_model


def _ensure_list(paths: Union[str, list[str]]) -> list[str]:
    if isinstance(paths, str):
        return [paths]
    return paths


def transcribe_audio(
    audio_paths: Union[str, list[str]],
    out_dir: str = config.TRANSCRIPT_STORAGE_PATH,
    language: str = "vi",
    batch_size: int = 4,
) -> list[str]:

    audio_paths = _ensure_list(audio_paths)
    if not audio_paths:
        logger.warning("No valid audio files provided to transcribe_audio")
        return []

    batched_model = _get_s2t_batched_model()
    batched_model.model.model.load_model()

    os.makedirs(out_dir, exist_ok=True)

    transcripts = []
    for audio_path in audio_paths:
        try:
            logger.info(f"Transcribing audio file: {audio_path}")

            segments, info = batched_model.transcribe(
                audio_path, language=language, batch_size=batch_size
            )

            transcript = ""
            for segment in segments:
                s, e, t = segment.start, segment.end, segment.text.strip()
                transcript += f"[{s:.2f}s - {e:.2f}s] {t}\n"

            transcripts.append(transcript)
            logger.info(f"Completed transcription for: {audio_path}")
        except Exception as e:
            logger.error(f"Error transcribing {audio_path}: {e}")
            transcripts.append("")
            continue

    filenames = []
    for audio_path, transcript in zip(audio_paths, transcripts):
        if transcript == "":
            continue
        audio_filename = Path(audio_path).stem
        transcript_path = os.path.join(out_dir, f"{audio_filename}.txt")
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(transcript)

        filenames.append(transcript_path)

    if batched_model.model.model.device == "cuda":
        batched_model.model.model.unload_model()
        torch.cuda.empty_cache()

    return filenames


### DEPRECATED CODE USING ORIGINAL WHISPER MODEL

# @lru_cache(maxsize=1)
# def _get_s2t_model() -> Whisper:
#     logger.info(f"Loading speech to text model: whisper-{config.SPEECH2TEXT_MODEL}")
#     return load_model(config.SPEECH2TEXT_MODEL, device="cuda")


# def _ensure_list(paths: Union[str, list[str]]) -> list[str]:
#     if isinstance(paths, str):
#         return [paths]
#     return paths


# def transcribe_audio(
#     audio_paths: Union[str, list[str]],
#     out_dir: str = config.TRANSCRIPT_STORAGE_PATH,
#     language: str = "vi",
#     use_fp16: bool = True,
# ) -> list[str]:

#     audio_paths = _ensure_list(audio_paths)
#     if not audio_paths:
#         logger.warning("No valid audio files provided to transcribe_audio")
#         return []

#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     model = _get_s2t_model()
#     model = model.to(device)

#     os.makedirs(out_dir, exist_ok=True)

#     transcripts = []
#     for audio_path in audio_paths:
#         try:
#             logger.info(f"Transcribing audio file: {audio_path}")

#             result = model.transcribe(audio_path, language=language, fp16=use_fp16)

#             transcript = ""
#             for seg in result["segments"]:
#                 s, e, t = seg["start"], seg["end"], seg["text"].strip()
#                 transcript += f"[{s:.2f}s - {e:.2f}s] {t}\n"

#             transcripts.append(transcript)
#             logger.info(f"Completed transcription for: {audio_path}")
#         except Exception as e:
#             logger.error(f"Error transcribing {audio_path}: {e}")
#             transcripts.append("")
#             continue

#     filenames = []
#     for audio_path, transcript in zip(audio_paths, transcripts):
#         if transcript == "":
#             continue
#         audio_filename = Path(audio_path).stem
#         transcript_path = os.path.join(out_dir, f"{audio_filename}.txt")
#         with open(transcript_path, "w", encoding="utf-8") as f:
#             f.write(transcript)

#         filenames.append(transcript_path)

#     if device == "cuda":
#         model.to(device="cpu")
#         torch.cuda.empty_cache()

#     return filenames
